{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cdd51ff",
   "metadata": {},
   "source": [
    "# Telco Churn Prediction (Logistic Regression)\n",
    "\n",
    "**Goal:** Predict customer churn (`churn` = 1) using a leakage-resistant feature set and evaluate the model with **ROC-AUC** and **PR-AUC**.\n",
    "\n",
    "**Inputs (provided):**\n",
    "- `telcodata_extract1.csv` (customer profile + usage/behavior)\n",
    "- `telcodata_extract2.csv` (label `churn`, `upsell_xsell`)\n",
    "- `telcodata_extract3.csv` (plan / device / lifecycle + call-center attributes)\n",
    "- `Data_dictionary_clean_combined.csv` (variable types + labels)\n",
    "\n",
    "**Outputs (generated by this notebook):**\n",
    "- `Telco_Churn_LogReg_Analysis.xlsx` (data audit + model results)\n",
    "\n",
    "> Note: We intentionally exclude variables that can cause **label leakage** (e.g., current-month charges, call-center intent fields).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Setup\n",
    "# =========================\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA_DIR = \"/mnt/data\"  # <- change if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b2d95",
   "metadata": {},
   "source": [
    "## 1) Helper functions (load/merge, cleaning, feature selection, modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c563ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_numeric_strings(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convert currency/commas/%/mixed strings to numeric; invalid -> NaN.\"\"\"\n",
    "    return pd.to_numeric(\n",
    "        s.astype(str).str.replace(r\"[^0-9\\.\\-]\", \"\", regex=True),\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def make_onehot_encoder(min_frequency: int = 200):\n",
    "    \"\"\"Compatibility helper: scikit-learn changed OneHotEncoder 'sparse' -> 'sparse_output'.\"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", min_frequency=min_frequency, sparse_output=True)\n",
    "    except TypeError:\n",
    "        # older scikit-learn\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", min_frequency=min_frequency, sparse=True)\n",
    "\n",
    "def load_and_merge(data_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load 3 extracts and merge into one modeling table + load data dictionary.\"\"\"\n",
    "    p1 = os.path.join(data_dir, \"telcodata_extract1.csv\")\n",
    "    p2 = os.path.join(data_dir, \"telcodata_extract2.csv\")\n",
    "    p3 = os.path.join(data_dir, \"telcodata_extract3.csv\")\n",
    "    ddp = os.path.join(data_dir, \"Data_dictionary_clean_combined.csv\")\n",
    "\n",
    "    df1 = pd.read_csv(p1, low_memory=False)\n",
    "    df2 = pd.read_csv(p2)\n",
    "    df3 = pd.read_csv(p3)\n",
    "    dd = pd.read_csv(ddp)\n",
    "\n",
    "    # Align key name\n",
    "    df3 = df3.rename(columns={\"cust_id\": \"Customer_ID\"})\n",
    "\n",
    "    # Merge all\n",
    "    df = df1.merge(df2, on=\"Customer_ID\").merge(df3, on=\"Customer_ID\")\n",
    "    return df, dd\n",
    "\n",
    "\n",
    "def prep_features(df: pd.DataFrame, dd: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str], List[str]]:\n",
    "    \"\"\"Clean numeric fields (based on data dictionary) and return type maps.\"\"\"\n",
    "    type_map = dd.set_index(\"variable\")[\"type\"].to_dict()\n",
    "    label_map = dd.set_index(\"variable\")[\"label\"].to_dict()\n",
    "\n",
    "    num_cols = [c for c in df.columns if type_map.get(c) == \"Num\"]\n",
    "\n",
    "    out = df.copy()\n",
    "    for c in num_cols:\n",
    "        if c in {\"Customer_ID\", \"churn\", \"upsell_xsell\"}:\n",
    "            continue\n",
    "        if out[c].dtype == \"object\":\n",
    "            out[c] = _clean_numeric_strings(out[c])\n",
    "\n",
    "    return out, label_map, num_cols\n",
    "\n",
    "\n",
    "def build_dataset(df: pd.DataFrame, dd: pd.DataFrame) -> Tuple[pd.DataFrame, List[str], List[str], List[str], pd.DataFrame]:\n",
    "    \"\"\"Select a leakage-resistant feature set for churn modeling.\"\"\"\n",
    "    dfc, label_map, _num_cols = prep_features(df, dd)\n",
    "\n",
    "    target = \"churn\"\n",
    "    exclude_always = {\"Customer_ID\", target, \"upsell_xsell\"}\n",
    "    all_features = [c for c in dfc.columns if c not in exclude_always]\n",
    "\n",
    "    # 1) Drop obvious \"current month\" fields (often leak churn status)\n",
    "    leak_cols = [\n",
    "        \"voice_tot_bill_mou_curr\",\n",
    "        \"tot_voice_chrgs_curr\",\n",
    "        \"mb_data_usg_m01\",\n",
    "        \"mb_data_usg_roamm01\",\n",
    "        \"tot_mb_data_curr\",\n",
    "        \"tot_mb_data_roam_curr\",\n",
    "        \"data_prem_chrgs_curr\",\n",
    "    ]\n",
    "    all_features = [c for c in all_features if c not in leak_cols]\n",
    "\n",
    "    # 2) Remove call-center intent fields (usually occur near/after churn decision)\n",
    "    call_center_like = {\n",
    "        \"nbr_contacts\",\n",
    "        \"calls_TS_acct\",\n",
    "        \"calls_care_acct\",\n",
    "        \"calls_care_3mavg_acct\",\n",
    "        \"calls_care_6mavg_acct\",\n",
    "        \"res_calls_3mavg_acct\",\n",
    "        \"res_calls_6mavg_acct\",\n",
    "        \"last_rep_sat_score\",\n",
    "        \"network_mention\",\n",
    "        \"service_mention\",\n",
    "        \"price_mention\",\n",
    "        \"call_center\",\n",
    "        \"issue_level1\",\n",
    "        \"issue_level2\",\n",
    "        \"call_category_1\",\n",
    "        \"call_category_2\",\n",
    "        \"resolution\",\n",
    "    }\n",
    "    all_features = [c for c in all_features if c not in call_center_like]\n",
    "\n",
    "    # 3) Remove suspension/status fields (often label-adjacent)\n",
    "    suspension_like = {\"count_of_suspensions_6m\", \"avg_days_susp\"}\n",
    "    all_features = [c for c in all_features if c not in suspension_like]\n",
    "\n",
    "    # 4) Remove sensitive demographic proportions (fairness/compliance)\n",
    "    sensitive_cs = {\n",
    "        \"cs_hispanic\",\n",
    "        \"cs_caucasian\",\n",
    "        \"cs_afr_amer\",\n",
    "        \"cs_other\",\n",
    "        \"cs_ttl_male\",\n",
    "        \"cs_ttl_female\",\n",
    "    }\n",
    "    all_features = [c for c in all_features if c not in sensitive_cs]\n",
    "\n",
    "    # 5) Remove ID-like/high-cardinality geography represented as numbers\n",
    "    all_features = [c for c in all_features if c not in {\"zipcode_primary\", \"state\"}]\n",
    "\n",
    "    # Identify numeric vs categorical by dtype (after cleaning)\n",
    "    num_features = [c for c in all_features if pd.api.types.is_numeric_dtype(dfc[c])]\n",
    "    cat_features = [c for c in all_features if c not in num_features]\n",
    "\n",
    "    # Optional: city is extremely high-cardinality; drop for interpretability\n",
    "    cat_features = [c for c in cat_features if c != \"city\"]\n",
    "\n",
    "    all_features = num_features + cat_features\n",
    "\n",
    "    # Build exclusion report\n",
    "    excluded = sorted(list(set([c for c in dfc.columns if c not in exclude_always]) - set(all_features) - set(leak_cols)))\n",
    "    excluded_df = pd.DataFrame(\n",
    "        {\n",
    "            \"variable\": excluded,\n",
    "            \"label\": [label_map.get(v, v) for v in excluded],\n",
    "            \"reason\": [\n",
    "                \"Leakage / not available before churn\"\n",
    "                if v in (set(leak_cols) | call_center_like | suspension_like)\n",
    "                else \"Removed for fairness/compliance\"\n",
    "                if v in sensitive_cs\n",
    "                else \"ID-like/high-cardinality\"\n",
    "                if v in {\"zipcode_primary\", \"state\", \"city\"}\n",
    "                else \"Excluded\"\n",
    "                for v in excluded\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dfc, all_features, num_features, cat_features, excluded_df\n",
    "\n",
    "\n",
    "def fit_logistic_regression(\n",
    "    dfc: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    num_features: List[str],\n",
    "    cat_features: List[str],\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[Pipeline, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Fit a regularized logistic regression model with preprocessing.\"\"\"\n",
    "    X = dfc[features]\n",
    "    y = dfc[\"churn\"].astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.30, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", make_onehot_encoder(min_frequency=200)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_features),\n",
    "            (\"cat\", categorical_transformer, cat_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        penalty=\"l2\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=300,\n",
    "    )\n",
    "\n",
    "    clf = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    proba = pd.Series(clf.predict_proba(X_test)[:, 1], index=y_test.index)\n",
    "    return clf, X_test, y_test, proba\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutputs:\n",
    "    roc_auc: float\n",
    "    pr_auc: float\n",
    "    baseline_rate: float\n",
    "    top10_threshold: float\n",
    "    top10_precision: float\n",
    "    top10_recall: float\n",
    "    confusion: np.ndarray\n",
    "    decile_table: pd.DataFrame\n",
    "    coef_table: pd.DataFrame\n",
    "    data_quality: pd.DataFrame\n",
    "    excluded: pd.DataFrame\n",
    "\n",
    "\n",
    "def summarize_model(\n",
    "    clf: Pipeline,\n",
    "    dfc: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    excluded_df: pd.DataFrame,\n",
    "    random_state: int = 42,\n",
    ") -> ModelOutputs:\n",
    "    \"\"\"Compute AUCs, lift/deciles, coefficients, and data-quality tables.\"\"\"\n",
    "    X = dfc[features]\n",
    "    y = dfc[\"churn\"].astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.30, random_state=random_state, stratify=y\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    roc_auc = float(roc_auc_score(y_test, score))\n",
    "    pr_auc = float(average_precision_score(y_test, score))\n",
    "    baseline = float(y_test.mean())\n",
    "\n",
    "    # Top 10% threshold\n",
    "    top10 = float(np.quantile(score, 0.9))\n",
    "    pred = (score >= top10).astype(int)\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    prec = float(precision_score(y_test, pred))\n",
    "    rec = float(recall_score(y_test, pred))\n",
    "\n",
    "    # Decile lift table (D10 highest risk)\n",
    "    t = pd.DataFrame({\"y\": y_test.values, \"score\": score})\n",
    "    t[\"decile\"] = pd.qcut(t[\"score\"], 10, labels=False, duplicates=\"drop\")\n",
    "    dec = (\n",
    "        t.groupby(\"decile\")\n",
    "        .agg(\n",
    "            customers=(\"y\", \"size\"),\n",
    "            churners=(\"y\", \"sum\"),\n",
    "            churn_rate=(\"y\", \"mean\"),\n",
    "            avg_score=(\"score\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(\"decile\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    dec[\"label\"] = [f\"D{i}\" for i in range(10, 0, -1)]\n",
    "    dec[\"lift\"] = dec[\"churn_rate\"] / baseline\n",
    "    dec[\"cum_churners\"] = dec[\"churners\"].cumsum()\n",
    "    dec[\"capture_rate\"] = dec[\"cum_churners\"] / dec[\"churners\"].sum()\n",
    "    decile_table = dec[[\"label\", \"customers\", \"churners\", \"churn_rate\", \"lift\", \"capture_rate\", \"avg_score\"]]\n",
    "\n",
    "    # Coefficients (standardized / one-hot)\n",
    "    feature_names = clf.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "    coefs = clf.named_steps[\"model\"].coef_.ravel()\n",
    "    coef_df = pd.DataFrame({\"feature\": feature_names, \"coef\": coefs})\n",
    "    coef_df[\"odds_ratio\"] = np.exp(coef_df[\"coef\"])\n",
    "    coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
    "    coef_df = coef_df.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Data quality\n",
    "    dq = pd.DataFrame(\n",
    "        {\n",
    "            \"variable\": features,\n",
    "            \"dtype\": [str(dfc[c].dtype) for c in features],\n",
    "            \"missing_pct\": [float(dfc[c].isna().mean()) for c in features],\n",
    "            \"n_unique\": [int(dfc[c].nunique(dropna=True)) for c in features],\n",
    "        }\n",
    "    ).sort_values(\"missing_pct\", ascending=False)\n",
    "\n",
    "    return ModelOutputs(\n",
    "        roc_auc=roc_auc,\n",
    "        pr_auc=pr_auc,\n",
    "        baseline_rate=baseline,\n",
    "        top10_threshold=top10,\n",
    "        top10_precision=prec,\n",
    "        top10_recall=rec,\n",
    "        confusion=cm,\n",
    "        decile_table=decile_table,\n",
    "        coef_table=coef_df,\n",
    "        data_quality=dq,\n",
    "        excluded=excluded_df,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_excel_report(out_path: str, dd: pd.DataFrame, outputs: ModelOutputs, sample: pd.DataFrame) -> None:\n",
    "    \"\"\"Create a multi-sheet Excel report (no external dependencies beyond openpyxl).\"\"\"\n",
    "    from openpyxl import Workbook\n",
    "    from openpyxl.utils import get_column_letter\n",
    "    from openpyxl.styles import Alignment, Font, PatternFill\n",
    "    from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "\n",
    "    header_fill = PatternFill(\"solid\", fgColor=\"0B1F3A\")\n",
    "    header_font = Font(color=\"FFFFFF\", bold=True)\n",
    "    center = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "\n",
    "    def style_header(ws, row: int) -> None:\n",
    "        for cell in ws[row]:\n",
    "            cell.fill = header_fill\n",
    "            cell.font = header_font\n",
    "            cell.alignment = center\n",
    "\n",
    "    def add_table(ws, ref: str, name: str) -> None:\n",
    "        tab = Table(displayName=name, ref=ref)\n",
    "        tab.tableStyleInfo = TableStyleInfo(name=\"TableStyleMedium9\", showRowStripes=True)\n",
    "        ws.add_table(tab)\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # README\n",
    "    ws = wb.create_sheet(\"README\")\n",
    "    ws[\"A1\"] = \"Telco churn â€” Logistic Regression report\"\n",
    "    ws[\"A1\"].font = Font(size=16, bold=True, color=\"0B1F3A\")\n",
    "    ws[\"A3\"] = f\"ROC-AUC: {outputs.roc_auc:.3f} | PR-AUC: {outputs.pr_auc:.3f} | Baseline churn: {outputs.baseline_rate:.2%}\"\n",
    "    ws[\"A4\"] = (\n",
    "        f\"Top 10% risk threshold: {outputs.top10_threshold:.3f} | \"\n",
    "        f\"Precision: {outputs.top10_precision:.1%} | Recall: {outputs.top10_recall:.1%}\"\n",
    "    )\n",
    "    ws.column_dimensions[\"A\"].width = 120\n",
    "\n",
    "    # Data dictionary\n",
    "    ws = wb.create_sheet(\"DATA_DICTIONARY\")\n",
    "    ws.append(list(dd.columns))\n",
    "    style_header(ws, 1)\n",
    "    for r in dd.itertuples(index=False):\n",
    "        ws.append(list(r))\n",
    "    add_table(ws, f\"A1:G{ws.max_row}\", \"DataDict\")\n",
    "\n",
    "    # Data quality\n",
    "    ws = wb.create_sheet(\"DATA_QUALITY\")\n",
    "    ws.append(list(outputs.data_quality.columns))\n",
    "    style_header(ws, 1)\n",
    "    for r in outputs.data_quality.itertuples(index=False):\n",
    "        ws.append(list(r))\n",
    "    add_table(ws, f\"A1:D{ws.max_row}\", \"DataQuality\")\n",
    "\n",
    "    # Excluded\n",
    "    ws = wb.create_sheet(\"EXCLUDED\")\n",
    "    ws.append(list(outputs.excluded.columns))\n",
    "    style_header(ws, 1)\n",
    "    for r in outputs.excluded.itertuples(index=False):\n",
    "        ws.append(list(r))\n",
    "    add_table(ws, f\"A1:C{ws.max_row}\", \"Excluded\")\n",
    "\n",
    "    # Performance\n",
    "    ws = wb.create_sheet(\"PERFORMANCE\")\n",
    "    ws.append([\"metric\", \"value\"])\n",
    "    style_header(ws, 1)\n",
    "    rows = [\n",
    "        (\"ROC_AUC\", outputs.roc_auc),\n",
    "        (\"PR_AUC\", outputs.pr_auc),\n",
    "        (\"Baseline churn\", outputs.baseline_rate),\n",
    "        (\"Top10 threshold\", outputs.top10_threshold),\n",
    "        (\"Top10 precision\", outputs.top10_precision),\n",
    "        (\"Top10 recall\", outputs.top10_recall),\n",
    "    ]\n",
    "    for k, v in rows:\n",
    "        ws.append([k, float(v)])\n",
    "\n",
    "    ws.append([])\n",
    "    ws.append([\"Confusion matrix (Top10)\", \"Pred 0\", \"Pred 1\"])\n",
    "    style_header(ws, ws.max_row)\n",
    "    ws.append([\"Actual 0\", int(outputs.confusion[0, 0]), int(outputs.confusion[0, 1])])\n",
    "    ws.append([\"Actual 1\", int(outputs.confusion[1, 0]), int(outputs.confusion[1, 1])])\n",
    "\n",
    "    # Deciles\n",
    "    ws = wb.create_sheet(\"DECILE_LIFT\")\n",
    "    ws.append(list(outputs.decile_table.columns))\n",
    "    style_header(ws, 1)\n",
    "    for r in outputs.decile_table.itertuples(index=False):\n",
    "        ws.append(list(r))\n",
    "    add_table(ws, f\"A1:G{ws.max_row}\", \"DecileLift\")\n",
    "\n",
    "    # Coefs (top 50)\n",
    "    ws = wb.create_sheet(\"COEFFICIENTS\")\n",
    "    top = outputs.coef_table.head(50).copy()\n",
    "    ws.append(list(top.columns))\n",
    "    style_header(ws, 1)\n",
    "    for r in top.itertuples(index=False):\n",
    "        ws.append(list(r))\n",
    "    add_table(ws, f\"A1:D{ws.max_row}\", \"Coefs\")\n",
    "\n",
    "    # Sample\n",
    "    ws = wb.create_sheet(\"SAMPLE_CLEAN\")\n",
    "    ws.append(list(sample.columns))\n",
    "    style_header(ws, 1)\n",
    "    for r in sample.itertuples(index=False):\n",
    "        ws.append(list(r))\n",
    "    last_col = get_column_letter(sample.shape[1])\n",
    "    add_table(ws, f\"A1:{last_col}{ws.max_row}\", \"Sample\")\n",
    "\n",
    "    wb.save(out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd59a15",
   "metadata": {},
   "source": [
    "## 2) Load data, build features, train Logistic Regression, compute AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load + merge\n",
    "df_raw, dd = load_and_merge(DATA_DIR)\n",
    "\n",
    "print(\"Merged dataset shape:\", df_raw.shape)\n",
    "print(\"Target distribution (churn):\")\n",
    "display(df_raw[\"churn\"].value_counts(dropna=False).to_frame(\"count\"))\n",
    "print(\"Churn rate:\", df_raw[\"churn\"].mean())\n",
    "\n",
    "# Build modeling dataset (feature selection + leakage control)\n",
    "dfc, features, num_features, cat_features, excluded_df = build_dataset(df_raw, dd)\n",
    "\n",
    "print(\"\\nSelected features:\", len(features))\n",
    "print(\" - numeric:\", len(num_features))\n",
    "print(\" - categorical:\", len(cat_features))\n",
    "print(\"\\nExcluded variables (count):\", excluded_df.shape[0])\n",
    "display(excluded_df.head(10))\n",
    "\n",
    "# Fit model + evaluate\n",
    "clf, X_test, y_test, proba_test = fit_logistic_regression(dfc, features, num_features, cat_features, random_state=RANDOM_STATE)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, proba_test)\n",
    "pr_auc  = average_precision_score(y_test, proba_test)\n",
    "\n",
    "print(f\"ROC-AUC (test): {roc_auc:.3f}\")\n",
    "print(f\"PR-AUC  (test): {pr_auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f624983",
   "metadata": {},
   "source": [
    "## 3) Business view: Top-decile targeting, lift & capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e3e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = summarize_model(clf, dfc, features, excluded_df, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Baseline churn rate:\", f\"{outputs.baseline_rate:.2%}\")\n",
    "print(\"Top 10% threshold:\", f\"{outputs.top10_threshold:.3f}\")\n",
    "print(\"Top 10% precision:\", f\"{outputs.top10_precision:.1%}\")\n",
    "print(\"Top 10% recall:\", f\"{outputs.top10_recall:.1%}\")\n",
    "print(\"\\nConfusion matrix (Top10 threshold):\")\n",
    "display(pd.DataFrame(outputs.confusion, index=[\"Actual 0\",\"Actual 1\"], columns=[\"Pred 0\",\"Pred 1\"]))\n",
    "\n",
    "print(\"\\nDecile lift table (D10 = highest risk):\")\n",
    "display(outputs.decile_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b6fc6",
   "metadata": {},
   "source": [
    "## 4) Feature importance (coefficients & odds ratios)\n",
    "\n",
    "> Logistic Regression is interpretable: coefficients show how features move churn odds (after preprocessing: numeric standardized, categorical one-hot).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top drivers by absolute coefficient\n",
    "display(outputs.coef_table.head(20))\n",
    "\n",
    "# (Optional) show the strongest positive drivers only\n",
    "top_pos = outputs.coef_table.sort_values(\"coef\", ascending=False).head(15)\n",
    "top_neg = outputs.coef_table.sort_values(\"coef\", ascending=True).head(15)\n",
    "\n",
    "print(\"Strongest POSITIVE drivers (increase churn odds):\")\n",
    "display(top_pos)\n",
    "\n",
    "print(\"Strongest NEGATIVE drivers (decrease churn odds):\")\n",
    "display(top_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b5541",
   "metadata": {},
   "source": [
    "## 5) Export Excel report (deliverable)\n",
    "\n",
    "This creates a multi-sheet Excel file you can attach to your submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bd9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_XLSX = os.path.join(DATA_DIR, \"Telco_Churn_LogReg_Analysis.xlsx\")\n",
    "\n",
    "# Include a small clean sample for review (first 200 rows)\n",
    "sample_cols = [\"Customer_ID\"] + features + [\"churn\"]\n",
    "sample = dfc[sample_cols].head(200).copy()\n",
    "\n",
    "build_excel_report(OUT_XLSX, dd, outputs, sample)\n",
    "\n",
    "print(\"Saved:\", OUT_XLSX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee3417",
   "metadata": {},
   "source": [
    "## 6) (Optional) Save churn risk scores for all customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on full dataset to score everyone (for deployment / campaign list)\n",
    "X_full = dfc[features]\n",
    "y_full = dfc[\"churn\"].astype(int)\n",
    "\n",
    "clf.fit(X_full, y_full)\n",
    "scores = clf.predict_proba(X_full)[:, 1]\n",
    "\n",
    "scored = pd.DataFrame({\n",
    "    \"Customer_ID\": dfc[\"Customer_ID\"].values,\n",
    "    \"churn\": y_full.values,\n",
    "    \"score\": scores\n",
    "}).sort_values(\"score\", ascending=False)\n",
    "\n",
    "OUT_SCORES = os.path.join(DATA_DIR, \"Telco_Churn_Scored.csv\")\n",
    "scored.to_csv(OUT_SCORES, index=False)\n",
    "\n",
    "print(\"Saved:\", OUT_SCORES)\n",
    "display(scored.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
